Plan for a Hybrid Federated Learning Framework

Overview and Objectives

We propose a hybrid federated learning framework that connects multiple clusters (data center servers) and resource-limited nodes (e.g. mobile or IoT devices) to collaboratively train machine learning models while preserving data privacy. The system will use federated learning (FL) with a hub-and-spoke topology: a central server (or cluster of servers) coordinates training, and many clients (devices or edge servers) perform local training on their private data ￼ ￼. No raw data leaves any cluster or device by default, only model updates are exchanged, ensuring that sensitive datasets remain local ￼. Key goals include:
	•	Federated Training across Clusters and Devices: Enable cross-silo FL (among server clusters) and cross-device FL (with mobile/IoT nodes) in one unified network ￼. The framework will scale from powerful data-center nodes to millions of lightweight devices ￼ ￼.
	•	Core Model + LoRA/QLoRA Training: Allow two modes of learning: (1) full foundation model training on powerful server clusters (updating all model parameters), and (2) lightweight LoRA/QLoRA fine-tuning on constrained nodes (updating only small adapter layers). This division lets edge devices personalize or extend the model with minimal resource usage, then merge those improvements into the global model.
	•	Continual Learning without Forgetting: Incorporate ideas from Google’s Nested Learning paradigm ￼ to treat the overall model as a hierarchy of nested learning problems. Each cluster or node’s training task can be seen as a nested sub-task with its own context, feeding into the global model’s learning process. This multi-level optimization design aims to avoid catastrophic forgetting when new data or tasks are learned ￼.
	•	Privacy and Security by Design: Ensure data sovereignty – each organization’s or device’s data stays within its own environment unless explicitly permitted. Use robust security controls (like Apache Ranger) and PII de-identification (Presidio) to enforce privacy policies. We will also integrate monitoring and versioning (with DataHub, MLflow) for transparency, and an observability stack for tracking the training process.

Below we detail the architecture, components, and process of this federated framework.

System Architecture and Components

Figure: Example of a federated learning architecture spanning cloud, edge, and end devices. A central server (cloud) coordinates training with multiple clusters or edge nodes, each in turn aggregating data from local devices. Our framework will connect powerful server clusters (for core model training) and individual edge devices (for LoRA fine-tuning) in a hybrid FL topology.

Federation Topology: We adopt a hub-and-spoke architecture typical of FL ￼. A Federation Server (or a cluster acting as the server) manages the global model state and coordination. Federation Clients can be of two types:
	•	Server-Clients (Clusters): These are powerful clusters (e.g. data center nodes with GPUs) that join the federation. A server-client can train the full foundation model (all weights) on its local dataset, or perform larger updates like full fine-tuning. It can also fine-tune LoRA adapters if desired. In essence, when a cluster participates as a client, it contributes heavy compute and large data.
	•	Edge-Clients (Devices): These are resource-limited nodes such as mobile phones, IoT devices, or edge servers. They will typically not train all model weights due to limited RAM/CPU/GPU. Instead, they download a compressed or partial model and perform LoRA/QLoRA fine-tuning on their local data (training only small adapter parameters) to reduce resource usage. Each edge client thus solves a smaller optimization problem (training a few thousand parameters) and sends those updates back.

Communication Layer: We leverage the Flower federated learning framework for networking and orchestration. Flower provides a robust server-client setup with support for heterogeneous clients (different devices, ML frameworks, etc.) ￼. It uses a long-running server process to broadcast tasks and collect results, and lightweight client SDKs that handle receiving the global model, training on local data, and returning gradients or updated parameters ￼. Communication is typically over gRPC or sockets, but we will also integrate a messaging/queue system: for example, the central server can post a “training round start” event to a message queue (e.g. via Kafka or RabbitMQ). Clients (which subscribe to these events) will then pull the latest model checkpoint from the server (or a model registry), perform training, and then push updates back (possibly via the Flower server or another queue topic for updates). This event-driven approach makes the system more robust and decoupled – clients can join/leave and handle intermittent connectivity, which is important for mobile nodes with unstable networks ￼.

Workflow Orchestration (Flyte): We use Flyte, a Kubernetes-native workflow orchestrator, to manage complex training pipelines across the system. Flyte can coordinate multi-step processes such as:
	•	Launching a new global training round (triggered perhaps by a schedule or new data arrival).
	•	Notifying clients (via the queue or Flower server) to start local training.
	•	Spawning jobs on each server-client cluster to run full model training using distributed PyTorch/DeepSpeed.
	•	Collecting client updates and running the aggregation function.
	•	Running evaluation tasks (either on a central validation set or distributed evaluation on clients).
	•	Registering the new model version (in MLflow/DataHub) and optionally deploying it or saving for next round.

Flyte workflows allow us to separate the pipelines for different tasks. For example, we can define one sub-pipeline for Core Model Training (executed when a powerful cluster is available with new core data) and another for LoRA Fine-Tuning (executed whenever an edge node has local data and comes online). Both pipelines feed into the same global model state but may be scheduled differently. Having dedicated pipelines ensures that the heavy training on clusters does not conflict with lightweight updates from edges – Flyte can queue and parallelize appropriately.

Core ML Frameworks: We will use PyTorch as the primary deep learning framework for model implementation. PyTorch gives us flexibility to define large language models (LLMs) or other neural networks, and it integrates well with both Flower (which supports PyTorch clients) and DeepSpeed. We will design the foundation model (global model) in PyTorch, and also incorporate Hugging Face Transformers and the PEFT (Parameter-Efficient Fine-Tuning) library for implementing LoRA/QLoRA on those models. This provides readily available utilities to apply LoRA layers to popular model architectures.

Distributed Training with DeepSpeed: On server clusters, training a large foundation model (which could be billions of parameters, e.g. a big LLM) is resource-intensive. We will integrate DeepSpeed to efficiently train and fine-tune large models across multiple GPUs/nodes. DeepSpeed’s ZeRO optimization partitions model states and gradients across GPUs to drastically reduce memory overhead ￼ – for example, ZeRO-Infinity can cut memory use by 10-100x compared to naive data parallelism ￼. This makes it feasible to train models with tens or hundreds of billions of parameters even on limited hardware. DeepSpeed can also offload memory to CPU or NVMe, enabling (for instance) a 10B+ model on a single GPU with ZeRO-Offload ￼. Using DeepSpeed in our framework means server clusters can train or fine-tune the core model efficiently, and even edge nodes can benefit: if an edge client has say 2 GPUs, DeepSpeed could help it fine-tune a smaller model or larger LoRA than otherwise possible. We will configure DeepSpeed for mixed precision and ZeRO Stage 3 to maximize throughput on clusters. When combined with LoRA (which only trains a few weight matrices), this allows us to train massive models with limited resources ￼. In fact, LoRA is known to “allow adaptively training massive models with limited resources” ￼ – our design leverages that by always using LoRA on the weakest devices.

Model Format and ONNX: To ensure portability of models between different environments, we will incorporate ONNX (Open Neural Network Exchange) format for serializing models. The core model trained in PyTorch can be exported to ONNX if needed for inference on devices that prefer ONNX runtimes. Within training, PyTorch/DeepSpeed will be used, but ONNX helps if, for example, a mobile app wants to run the model or a smaller distilled version thereof. We may not heavily use ONNX during training, but it’s a valuable part of the framework for deployment and compatibility. Additionally, if we ever choose to do inference on-device (for evaluating model quality on a client), having an ONNX model might enable using efficient runtimes like ONNX Runtime or Core ML.

Tool Integrations: The framework will integrate a suite of tools to manage data, security, and models:
	•	Apache Ranger: We use Ranger as a central component for data access control and governance. Ranger will enforce policies so that no sensitive data leaves its cluster. For instance, if a cluster node tries to send raw data to the server, Ranger policies can prevent that unless explicitly allowed. Ranger provides fine-grained authorization on data access and a central admin UI for security policies ￼ ￼. We will configure Ranger on each cluster’s data storage (e.g., HDFS, databases, object stores) to allow the local training processes to read data but deny any external requests for that data. This way, even though we orchestrate across clusters, each cluster’s data remains siloed and protected by Ranger’s auditing and access control rules.
	•	Microsoft Presidio: Presidio will be integrated into the data pipeline for PII detection and anonymization. Presidio can automatically scan text (and even images) to find sensitive personal data like names, addresses, phone numbers, etc., and redact or mask them ￼. In our framework, Presidio is useful if occasionally we need to log or transfer any data between nodes (for example, if an organization opts in to share a small sample for debugging or if we use a federated analytics approach). Before any such transfer, Presidio’s analyzers will detect PII and its anonymizer will remove or encrypt those elements ￼ ￼. This ensures compliance with privacy regulations and that even optional data sharing or global validation sets contain no personal identifiers. Presidio being modular and language-agnostic fits well as a plug-in to our preprocessing pipeline ￼. For instance, an edge device could run Presidio on its logs to ensure nothing sensitive is inadvertently sent in model updates or error reports.
	•	DataHub: We incorporate LinkedIn’s DataHub as a central metadata catalog and lineage tracker for our ML assets. DataHub will track datasets, models, training runs, and their relationships. For example, each cluster can register its local dataset in DataHub (just the metadata, not the data itself), and we can document which model versions were trained on which data. DataHub has built-in support for ML metadata: experiments, model groups, versions, etc ￼ ￼. We will integrate DataHub with MLflow (which we use for model versioning – see next section) so that whenever a new model version is logged, DataHub is updated with a searchable record of that model and its lineage. This helps in governance (knowing which data influenced the global model) and discoverability (new clients can see what models or LoRA adapters are available). DataHub’s integration with MLflow means the runs/experiments and model registry entries can be automatically captured ￼ ￼. Overall, DataHub provides the “central place to organize and track AI assets”, making experiments, models, and their metadata easily accessible across the team ￼.
	•	MLflow: We use MLflow for experiment tracking and model registry/versioning. Each training round or fine-tuning run can be an MLflow experiment run, logging parameters (hyperparameters, client IDs, data size, etc.), metrics (training loss, evaluation accuracy), and artifacts like model weights or LoRA weight files. When a global model is produced (after aggregating a round of federated updates), we register it as a new model version in MLflow’s Model Registry. MLflow provides an API to fetch the latest model or a specific version, which is useful for clients when downloading the model. We’ll also log LoRA adapter weights as separate artifacts – e.g., if an edge device fine-tunes a LoRA, that small adapter can be saved and versioned. Over time, we might accumulate many LoRA adapters (for various tasks or clients); MLflow and DataHub together will help keep track of these. For example, DataHub’s ML Model entities will map to MLflow’s model versions ￼, so one can query “Model X version 5 – which data and which LoRA updates went into this?”. This rigorous versioning helps with continual learning, as we can roll back if an update degraded performance or compare different strategies. It also supports CI/CD for models – if a new model version is registered, we could trigger tests or deployment (though deployment is outside scope of training framework, it’s enabled by having a registry).

Federated Training Process (Server-Client Interaction)

With the architecture and components established, the training process will work as follows for each round or training task:
	1.	Global Model Initialization: The central server either initializes a base model (randomly or from a pre-trained checkpoint) or loads the latest global model from the registry. This model may be large (for cluster training) but we can also maintain a quantized copy for edge use. For example, the server could maintain a 16-bit weight version for itself and a 4-bit quantized version for devices (to use with QLoRA).
	2.	Server Broadcast (Event Queue): When a training round or update is to occur, the server creates a task message placed onto a queue/topic that all clients listen to. The message might include: model version ID, required task (e.g. “train for N epochs on your local data with learning rate X”), and whether it’s full model training or LoRA. The Flower server can handle this broadcast inherently ￼, but by using a message queue we add reliability (clients that were offline can later read the message and catch up). Each client retrieves this event. The system may do client selection – e.g., only a subset of clients participate each round (common in FL to improve efficiency). In that case, the server or scheduler will target messages to specific clients or include a list of selected client IDs in the broadcast.
	3.	Model Distribution: Upon receiving the round start event, clients download the required model checkpoint. Server-clusters (powerful clients) will fetch the full model (e.g., a PyTorch state dict) for training. Edge-clients (devices) will fetch a reduced version: either the full model in a compressed form (like 4-bit weights for QLoRA) or possibly just the base model and a template LoRA adapter. We plan to use QLoRA on devices: this means the device loads a 4-bit quantized copy of the global model into memory ￼ ￼, attaches a fresh LoRA adapter (small trainable matrices), and trains only those. QLoRA dramatically cuts down memory usage by quantizing the model parameters to 4-bit precision, often reducing memory by 4× while maintaining model performance ￼ ￼. This allows, for example, a 65B-parameter model to be fine-tuned on a single GPU with 48GB memory ￼ – or on smaller devices, a multi-billion parameter model can be handled. Each client will also load its local training data (which is stored in a JSONL or similar format for LLM fine-tuning, as specified). The data can be preprocessed on the client side (and can be cached or streamed if large).
	4.	Local Training (on Clients): Now each client (cluster or device) performs local training. For a server-client (cluster node), this could involve running a distributed PyTorch job (or using DeepSpeed) across multiple GPUs on that cluster. The job will train the core model weights on that cluster’s local dataset for a certain number of epochs or steps. We may choose to freeze some layers if the model is extremely large, but generally server-clients can do full fine-tuning or even continue pre-training if new raw data is available. For an edge-client (device), the training is LoRA fine-tuning: the device’s model weights remain mostly frozen (aside from perhaps layer norms or a few unfrozen biases depending on strategy), and only the LoRA adapter’s weights (the low-rank matrices inserted into each layer) are updated ￼. This drastically reduces computation and memory – only e.g. 0.1% of the total parameters might be trainable. The device trains these via standard gradient descent on its local examples (e.g. using an optimizer like AdamW but with far fewer parameters to update). Because devices may have very little compute, they might only do a few epochs or even just one pass (depending on how FL is structured, sometimes federated averaging does one local epoch). We will tune the local epoch count per client type.
	5.	Privacy Enhancements During Training: To further ensure privacy, we will incorporate techniques like secure aggregation and differential privacy at this stage (if required by use case). Secure aggregation means the server only sees the combined updates of many clients, not any individual’s update in plaintext ￼. Our framework can employ Flower’s support for secure aggregation protocols (if available) or integrate an encryption step before clients upload updates (e.g., using homomorphic encryption or simple additive masks). Differential privacy (DP) can be used on the client side by adding noise to the gradients or model updates such that no individual data point’s influence can be reverse-engineered ￼. These are advanced features we will keep as configurable options for high-security settings. At minimum, no raw data leaves the client, and any personal identifiers in model updates (which is rare, but training updates can sometimes reveal data) could be mitigated by DP noise. Presidio’s role is mostly in preprocessing, but it also can scan model update payloads if we convert them to some intermediate representation (however, that’s experimental; mostly, we rely on the math of DP for model update privacy).
	6.	Upload of Updates: After local training, clients will send their model updates back to the server. In standard FL, what is sent are either the new model weights or the computed gradients. We will likely use Federated Averaging (FedAvg) as the base aggregation algorithm: clients send either the difference from the initial model or their new model parameters. Since sending the entire model can be heavy (especially if many clients), we might use adaptive approaches: for server-clusters, sending full weight updates is fine (they have good bandwidth), but for mobile devices, we can send only the LoRA adapter weights (which are very small, e.g. a few MB). The LoRA updates from each device are thus succinct. The central server will receive many LoRA updates, each essentially representing a small model that, when merged with the base model, accomplishes the client’s local task.
	7.	Aggregation and Core Model Update: The server now performs aggregation of the received updates. For full model updates from clusters, this can be an averaging weighted by data size (the classical FedAvg formula). For LoRA updates from devices, we need a strategy to integrate them into the core model without losing information. A simple approach is: treat each LoRA as a directional update and sum them into the base model (perhaps with a small learning rate). However, applying many LoRA deltas directly could cause interference. Another approach: the server can sequentially apply LoRAs and fine-tune the base model to absorb each one’s knowledge. A more principled approach could leverage the Nested Learning concept – essentially treat each LoRA adapter as learning a new skill, and incorporate it as an additional module in the model rather than overriding weights. For example, we can maintain multiple adapters inside the model for different data clusters and activate them as needed (this is like Mixture-of-Experts or multi-domain adapters). But to keep it simple: initially we will aggregate by converting LoRA updates to weight updates. Since LoRA adapters are low-rank, one technique is to average the adapter weights from multiple devices (weighted by some measure of data or trust) and then merge that average adapter into the base model weights (merging a LoRA into a model’s weights is essentially adding delta = A * B^T to the weight matrix). This yields a new global model that reflects an average of all device contributions. We might also maintain the adapter weights separately for later analysis or fine-tuning. If server clusters provided full model updates, those can be averaged straightforwardly. The aggregation logic can be customized via Flower’s Strategy API, where we can define how to combine model parameters from heterogeneous clients. Flower allows custom aggregation functions – for example, we could have one that knows if an update is LoRA form or full weights and handles accordingly.
	8.	Nested Learning for Continual Updates: Importantly, to avoid catastrophic forgetting as new updates come in, we plan the aggregation in a way inspired by Nested Learning. Traditional FL just averages weights, which could dilute a model’s prior knowledge when new task-specific updates arrive. Instead, viewing it as nested optimization problems ￼, we consider each client’s update as optimizing a sub-problem (its local objective) that should be incorporated without wiping out past solutions. In practice, this could mean multi-level training: e.g., while aggregating, the server could retrain the global model slightly on past knowledge or a replay buffer to reinforce old tasks. Another idea: the global model could have internal context flows corresponding to different knowledge – akin to how Nested Learning says each “level” has its own update flow ￼. Concretely, our framework might implement a two-tier optimization: the fast tier (inner loop) are the LoRA updates solving immediate tasks on nodes, and the slow tier (outer loop) is updating the base model in a way that minimizes forgetting. For example, the server can periodically evaluate the global model on old task data (if available in a validation set) and if performance drops, it could apply regularization or reuse some adapters from earlier rounds. By treating the architecture and training algorithm as unified levels ￼, we can design the model to naturally accommodate new information. One practical technique is to freeze a portion of the model (say, first layers) so that not all knowledge is overwritten, and allow new info to be absorbed in later layers or adapters. Another is using Elastic Weight Consolidation (EWC) or regularization during server aggregation to reduce drifts. These ideas align with Nested Learning’s goal of mitigating forgetting by structure. In summary, our aggregation phase will be careful to integrate new knowledge from LoRA updates while preserving existing capabilities – fulfilling the Nested Learning principle of a model as a set of nested sub-models, each solving different contexts ￼.
	9.	Model Versioning and Logging: Once the server obtains the new global model (after aggregation/adaptation), it will log this as a new model version. Using MLflow, we log the parameters of this round (e.g. how many clients, total data used, any hyperparameters for aggregation), evaluation metrics (global accuracy, loss), and register the global model weights. If the update included merging LoRAs, we might also log the merged adapter or keep a copy of each adapter. DataHub will capture this event: linking the new model version to all the input datasets (through client lineage) and to the previous model version (lineage of model). We also emit an event (via message queue or Flower) to clients that a new global model is available (so they can update their local copy for future use or for inference).
	10.	Iterate Next Round: The above steps will repeat for many rounds. Federated learning often needs multiple rounds to converge to a high-quality model, especially if clients only do small local training steps. Our framework will iterate until a stopping criterion: e.g. validation accuracy is met or a max number of rounds. During these rounds, different mixes of clients (clusters/devices) might participate based on availability. The hybrid aspect means sometimes we might have a round where only big clusters contribute (e.g. a round of full model pre-training on fresh data), and other times a round where thousands of phones contribute (a round of personalization updates). The framework should accommodate both seamlessly. In some cases, we could do hierarchical FL: if there are many devices per cluster, an edge server could first aggregate its devices’ updates, then send an intermediate update to the global server (to reduce bandwidth). This idea of an edge aggregator fits into the architecture (it would be similar to the “Edge Nodes” in the figure and Google’s cloud-edge-end paradigm). Flower can be extended to multi-tier aggregation, or we can handle it with an intermediate step in Flyte (i.e., have one Flyte workflow that aggregates device LoRAs per cluster, possibly even fine-tunes a cluster adapter, then the central server aggregates those). This will drastically reduce communication cost and make the system more scalable.

Throughout this process, monitoring and error handling are active: If a client fails or drops out (common with mobile nodes), the server notes it and can proceed with partial updates (Flower typically can continue with available results). Flyte’s orchestration can include retries or skip logic if some scheduled jobs (like a cluster training job) fails – ensuring the whole pipeline doesn’t stall.

Data Privacy and Security Measures

Data privacy is paramount in our framework. We enforce privacy on multiple levels, combining policy-based controls, encryption, and anonymization:
	•	Data Never Leaves Local Storage: By design, the raw datasets on each cluster or device are never uploaded to the central server ￼. All training happens locally. This addresses regulatory requirements like GDPR data locality – e.g., user data on a phone stays on the phone. If for any reason we need combined analysis of data, we would use federated analytics techniques (computing aggregate statistics without raw data) rather than centralized collection.
	•	Apache Ranger Policies: We deploy Apache Ranger on each cluster’s data platform to police data access. Ranger will log and control any access to sensitive data. For example, if a developer accidentally writes code to copy cluster data to an external location, Ranger can be configured to block that. It provides a centralized way to manage permissions: only the federated training service account has read access to the data, and that service is restricted to operate locally ￼ ￼. We also leverage Ranger’s audit logs – any attempted access is recorded, so we have an audit trail proving that data wasn’t improperly accessed. In essence, Ranger ensures that “data security within each silo is enforced with fine-grained authorization and auditing”, aligning with our goal that no data leaves without permission ￼.
	•	Secure Communication: All network communication between server and clients will be encrypted (TLS). Certificates or keys will be managed so that only authenticated clients can join the federation. We might integrate with an IAM system or Flower’s authentication to ensure we know which node is connecting. This prevents eavesdropping and also unauthorized rogue clients (which could try to inject bad updates). We can also sign the model binaries to ensure clients only run authorized model code.
	•	Differential Privacy (optional): As mentioned, we can add differential privacy noise to updates. This would be configured per use-case. If enabled, each client (especially for user devices) will clip and perturb their gradients before sending, so that individual user information is mathematically protected ￼. The trade-off is slight reduction in accuracy for a strong privacy guarantee. This is a cutting-edge approach to privacy which we will include as an opt-in module using libraries like PyTorch Opacus or TensorFlow Privacy as needed.
	•	PII Anonymization with Presidio: Before any data ever leaves a cluster (if it must), we run Presidio. For example, imagine we want to do a one-time central evaluation on a sample of user prompts. We could have clients send a few text examples. Presidio’s Analyzer would detect any names, phone numbers, etc., and the Anonymizer would mask or replace them with synthetic tokens ￼ ￼. This way even the central system never sees real PII, only sanitized content. Similarly, any logs or error traces collected from clients will go through Presidio (which can be packaged as a microservice with a REST API ￼). The fact Presidio is “modular and customizable” ￼ means we can tailor it to our data domain – e.g., if this framework is used in healthcare, we can add medical ID regex patterns to Presidio to catch those. By integrating Presidio deeply, we ensure compliance with privacy standards (HIPAA, etc.) when debugging or monitoring.
	•	Federated Analytics and Monitoring: We might want aggregate insights like “what is the distribution of labels across clients” or “how many have a certain data characteristic” without violating privacy. We will use Federated Analytics techniques ￼: essentially, have clients compute the needed statistic locally and send only aggregated or DP-noised results. For example, counting how many devices saw a particular rare event, each device sends a count and the server sums them (with secure agg so it doesn’t see individual counts). This avoids any individual’s data being exposed, yet gives global insight for model monitoring.
	•	Poisoning and Security Mitigations: A different aspect of privacy/security is ensuring a malicious client cannot corrupt the model (data poisoning or model poisoning attack). To address that, the server will include anomaly detection on updates. For instance, if one client’s update is extremely divergent or degrades performance on a validation set drastically, we can discard or downweight it. We can also maintain a hold-out dataset of known tasks to detect if the model’s behavior shifts unexpectedly after aggregation (which could indicate a poisoning attempt or a buggy update). This is not exactly privacy, but it’s security for model integrity – important for a robust framework.

In summary, our framework endeavors to “build privacy into its design”, combining technical safeguards (encryption, DP) with governance tools (Ranger, Presidio) to ensure that each participant’s data is protected and used only for its intended purpose.

Core Model Training vs LoRA Fine-Tuning Pipelines

A distinctive feature of our framework is the separation of two training pipelines – one for the core foundation model and one for the LoRA/QLoRA fine-tunings – both feeding into the federated loop. Here’s how they differ and operate:
	•	Core Model Training Pipeline: This pipeline is activated when a server cluster with substantial data wants to improve the foundation model’s general knowledge. For example, suppose Cluster A has a large new dataset (perhaps many terabytes of text or user interactions). Instead of doing standard FL (which might take many small rounds), we can have Cluster A perform an extended training job on the core model. This might resemble traditional centralized training but confined to that cluster. Using Flyte, we define a workflow that: loads the latest global model checkpoint, runs a distributed training job on Cluster A’s GPUs using PyTorch+DeepSpeed (possibly for multiple epochs, as this is like a mini pretraining), then evaluates and sends the updated model to the central server. Because this is a bigger update, we might label it a new major version of the model. Other clusters or devices could be idle or doing their own thing during this time – that’s fine. Once Cluster A is done, the central server can aggregate that as a large update (essentially the global model is now this new version). Optionally, the server could average it with the previous global model to moderate the change. The key point is this pipeline deals with full-model weight updates, potentially large-scale, and will likely rely on DeepSpeed for efficiency. It uses a dedicated data pipeline to feed data from the cluster’s storage to the training process (e.g., using PyTorch DataLoader, possibly reading from a data lake). We ensure this pipeline is optimized for throughput (caching, sharding across GPUs, etc.). Data format here can be any (CSV, Parquet, etc.), but for LLM fine-tuning we expect JSONL as a common format (each line a JSON with fields like prompt, response, etc.). We will incorporate conversion or loading of JSONL via Hugging Face Datasets or similar.
	•	LoRA Fine-Tuning Pipeline: This pipeline triggers whenever edge devices or any client chooses to fine-tune via LoRA. It might be running continuously in the background, listening for new events (like user feedback that could improve the model on that device). The steps are: attach LoRA adapter to local model, train on local data, and then instead of merging it locally, send the LoRA weights to the server. On the server side, we have a pipeline that collects these LoRA adapters. We have some options here. We could immediately merge each LoRA into the global model as they arrive (asynchronous FL) – but merging one by one might cause interference if order varies. A better approach is to accumulate multiple LoRAs and do a batch aggregation periodically. For instance, every day the server takes all LoRA updates received and averages or sequentially integrates them, creating a new global model. In the meantime, we could also distribute interim improvements back to devices (so they benefit from each other’s knowledge). This pipeline is thus event-driven and lightweight per update (LoRA files are small, often a few MBs even for big models). We might use Flyte to schedule an aggregation task for LoRAs when a threshold of updates is reached or a time interval passes. Additionally, the LoRA pipeline can have a data pipeline segment: some devices might have streaming data (like interactions) which need to be formatted into training examples (JSONL) on the fly. We might use a lightweight on-device process (or send the data through an edge node for preprocessing) to accumulate micro-batches of data before fine-tuning. Ensuring consistent formatting (e.g., proper prompt-response pairs with tokens) is important so that different devices’ contributions are compatible.

The separation of pipelines ensures that the needs of each are met: core training might be infrequent but heavy (requiring lots of computation time, careful hyperparameter tuning, maybe different learning rate schedules), while LoRA fine-tuning is frequent but light and can be done opportunistically (e.g., when a device is charging or when network conditions allow upload of results). By segregating them, we can also store different metrics: for core training we track metrics relevant to general performance, for LoRA we might track metrics on the local task. Ultimately, both pipelines converge in the model registry: whether a model update came from a big cluster job or from federated LoRAs, it becomes a new global model version.

Another advantage of this design is modularity. In future, if a new efficient fine-tuning method emerges (besides LoRA), we can slot it into the “edge pipeline”. Or if we switch the core model training to a different method (say, adding reinforcement learning steps or knowledge distillation), it stays encapsulated in the core pipeline. The framework coordinates both, but they don’t impede each other. This is inspired by the Nested Learning view – multiple learning processes at different scales, each with their own context, running in parallel ￼. The core model training could be seen as an outer loop providing general knowledge, and the LoRA fine-tunings as inner loops solving specific contexts (e.g., personalizing to a user’s style) – all nested within the overall model. By designing pipelines this way, we’re effectively implementing a multi-level learning system, which is more powerful than a single loop and helps avoid forgetting: the base model remains mostly stable from core training, while LoRAs handle new info without overwriting the base, then are carefully merged.

Incorporating Nested Learning for Continual Improvement

A key design influence is the concept of Nested Learning ￼, which reimagines a neural network not as one monolithic learner, but as a series of nested optimization problems with their own internal workflows. We explicitly build our framework to leverage this idea for continual learning:
	•	Multi-Level Optimization: In our federated setup, we identify at least two levels of learning: (1) Global optimization of the foundation model across all data, and (2) Local optimization on each client for its specific data/task. Instead of treating these as the same type of update, we handle them at different “speeds” and scopes. The local (nested) learning happens in many parallel small problems (each client trains its adapter or model on its local context), while the global learning happens as an overarching problem of integrating these without forgetting previous knowledge. This aligns with Nested Learning’s premise of simultaneous interconnected learning flows ￼. Each client’s LoRA training can be seen as a small internal workflow with its own context flow (its private data) and update frequency, nested within the larger model’s training ￼. The global model update is a higher-level workflow that proceeds more slowly and deliberately. By optimizing these levels together (with our strategy of careful aggregation and occasional global re-training), we approximate the Nested Learning paradigm in practice.
	•	Avoiding Catastrophic Forgetting: A common challenge in sequential or federated learning is that learning something new can degrade performance on old tasks (catastrophic forgetting) ￼. Our framework tackles this by never fully overwriting the model with a single client’s knowledge. LoRA updates, for example, keep the original weights intact, so a client can learn a new skill without erasing the old weights – this is a form of sparsity in the weight updates that helps preserve prior knowledge. When we merge adapters, we do it in a way that is averaged or regularized, rather than blindly setting weights to new values. We can also maintain previous adapters: if an old task needs to be revisited, we could reapply an old LoRA. This is analogous to the brain retaining specialized neurons for older memories. In fact, one could think of each LoRA adapter as a “memory” of a particular data concept. The Nested Learning paper’s solution “Hope” architecture was a self-modifying model that could retain long-term knowledge ￼. In our case, the combination of foundation model (long-term memory) and LoRA adapters (short-term, task-specific memory) serves a similar role. Over time, if certain LoRAs prove generally useful, we might merge them permanently into the base model; if not, we might keep them separate or even enable dynamic routing (choose adapter based on input). All these strategies ensure new learning doesn’t simply overwrite old parameters, thus mitigating catastrophic forgetting by structural design ￼.
	•	Context Flows: Each nested learning problem has its own context flow, as the paper describes ￼. In our framework: the context for a local training is the client’s local dataset (and any prior model state the client has). The context for the global training is the union of knowledge from all clients, which we access only through their updates. By treating the global model’s knowledge as compressed context (since it doesn’t see raw data), we effectively compress internal context flows as needed ￼. The separation of pipelines mentioned earlier ensures the context of core training (broad, diverse data) flows mainly when a cluster engages in outer-loop training, whereas the context of specialized tasks flows through LoRA on the inner loops. This is a concrete manifestation of nested context flows feeding a single model.
	•	Continuous Self-Improvement: The ultimate promise of Nested Learning is a system that continuously learns new tasks without forgetting old ones, much like human learning ￼ ￼. Our framework is built for continual learning – clients can join at any time with new data, and the model will incorporate it. To ensure it truly self-improves, we include model validation checks each round: using a hold-out or public dataset, we verify that performance on older tasks hasn’t regressed after integrating new updates. If it has, we can invoke remedial actions (like blending the old model weights more or fine-tuning a bit on some old data – since in cross-silo FL we might have some shared reference data or synthetic data). We are effectively treating the training process itself as an optimization where we want to minimize loss on new data and not increase loss on old data – a multi-objective that Nested Learning encourages us to think about. If we formalize it: each client solves min(local_loss), the server solves min(global_loss + λ * old_loss_penalty). Techniques like elastic weight consolidation (EWC), knowledge distillation from old model to new, or keeping a small rehearsal dataset can all be seen as implementing that penalty to avoid forgetting. We will experiment with these as needed. The framework’s flexibility (via Flyte orchestration) allows adding such steps if a drop in performance is detected.

In summary, by structuring training into nested levels (global vs local), using adapters to isolate new knowledge, and adding measures to protect old knowledge, our framework embodies the Nested Learning paradigm ￼. This will enable continual learning – the model will evolve over time as new nodes (or new data on existing nodes) come online, all while maintaining or even improving on previously learned tasks. The result should be a more resilient, capable AI model that approaches the human brain’s ability to accumulate skills over a lifetime without forgetting basics ￼.

Model Versioning and Experiment Tracking

To manage the evolving models and configurations, we have integrated comprehensive model versioning and experiment tracking tools:
	•	MLflow for Runs and Models: Every training run (be it a full model training on a cluster or a federated round) will be tracked as an MLflow run. This will record parameters (like learning rates, number of clients, etc.), metrics (loss curves, accuracies), and artifacts (models, logs). For federated rounds, we might log some summary metrics per client as well (Flower can return metrics from clients after each round, which we can save). When a new global model is produced, we use MLflow’s Model Registry to version it. For example, we may have a Model Registry entry called “HybridFL_LLM” with versions 1, 2, 3, … as the model evolves. Each version in MLflow is linked to the run that produced it, giving full traceability. We’ll assign stage labels like “Staging” or “Production” if this were to be deployed. MLflow also allows easy loading of a model by version for usage – e.g., when clients connect, they can query “give me the latest model version” via MLflow API and get it (assuming connectivity). This automates distribution of models beyond just relying on the event message with a download link.
	•	DataHub for Metadata and Lineage: As mentioned earlier, DataHub complements MLflow by providing a unified metadata view. Where MLflow is focused on model files and metrics, DataHub will capture the relationships: which dataset was used by which run, which run produced which model, how models group together. For example, DataHub’s ML Model Group concept can group all versions of our model together ￼, and each version can carry tags or properties (like “trained with 20% new data from cluster B”). The lineage features mean we can ask “what data did version 5 of the model use?” and get the answer (because DataHub will have an entity for the training run linking to dataset entities for each client’s data) ￼ ￼. We will programmatically update DataHub via its API or integration hooks whenever a new run happens. DataHub also stores features of data (schema, etc.) and can thus be used to ensure that if data schemas change, we know which model might be affected. Essentially, DataHub plus MLflow gives us end-to-end visibility into the ML lifecycle – crucial for debugging and compliance. Since our system will likely be long-running and produce many models, this prevents it from becoming a black box.
	•	Model Versioning Strategy: We plan to differentiate between major and minor version updates. A major version might correspond to a significant core model update (like after a full re-training on a big new dataset or architecture change), while minor versions could correspond to incremental updates from federated rounds of LoRA integration. For example, Model 2.0 (major) might be after adding a new capability via cluster training, then 2.1, 2.2, 2.3… might be successive daily federated updates from edge devices. This version semantics can be implemented by using the version numbering in MLflow or using tags (e.g., tag a run as major_update=True). The reasoning is to allow easier rollbacks if needed: if a minor update introduces a regression, we can roll back to the previous major version quickly. Automated versioning means every model that goes into production or further training is saved – nothing is lost – which is important for auditability (some regulations require knowing exactly what model was in use at a given time).
	•	Automated Experimentation: The framework can also support automated experiments. Suppose we want to test a new aggregation algorithm or a different hyperparameter. Using MLflow, we can run an A/B test: e.g., run two federated learning experiments in parallel with different settings (Flower can partition clients for two separate instances). Each experiment logs to MLflow separately. We compare metrics and decide which approach yields better accuracy or fairness. The combination of MLflow and DataHub can help here by organizing experiments and making it easy to compare them (MLflow UI allows comparing runs metrics side by side).
	•	Artifact Storage: All these model weights (especially if we have many versions) need storage. MLflow by default can save to a local or cloud storage. We will likely use a centralized object store (like S3 or HDFS) for storing model artifacts. Since our models could be large (multi-GB), we must budget storage and perhaps periodically clean up older artifacts that are superseded (though maybe we keep all to be safe). DataHub will store lightweight metadata, not the heavy files, so it’s fine. We might also use DVC (Data Version Control) or similar for dataset versioning if needed, but since data stays local, DataHub’s dataset metadata might be enough.
	•	Integration with CI/CD: With model versioning in place, if this framework is part of a product pipeline, we can connect it to deployment pipelines. For instance, once a model version is marked “Production” in MLflow, an automated job could convert it to ONNX and deploy to an inference service. Or if a new version is created, we could run a batch of evaluation jobs (again orchestrated by Flyte) and only promote the model if it passes metrics thresholds. These sort of automation closes the loop from federated training to real-world use. While not the main focus, our design facilitates it.

In summary, every step of model development is tracked and versioned. Researchers and engineers can later discover models via DataHub (since DataHub enables search like “find me models trained on Dataset X” ￼) and reproduce results by checking MLflow logs. This rigor ensures that as the framework runs continuously, we don’t lose track of what happened – a common risk in long-lived federated learning processes.

Monitoring and Observability

To operate this federated system at scale, we need robust monitoring and observability tools covering both infrastructure metrics and ML metrics:
	•	System Monitoring (Infrastructure): Each cluster and device should be monitored for resource usage (CPU, GPU utilization, memory, disk, network) to identify bottlenecks. We will use Prometheus as a metrics collector on the server side, scraping metrics from cluster nodes. For devices, full Prometheus clients may be heavy, but we could have the edge node (or the Flower client runtime) report simple stats to the server which then exposes them. We’ll set up Grafana dashboards that display the health of the federation: number of active clients, throughput of the message queue, time taken for each training round, etc. For example, we can chart how long each round takes from broadcast to aggregation, to ensure it’s within expected bounds. If a certain client (or cluster) is consistently slow or failing, that would appear in the metrics (like it might not submit updates, or its training time metric spikes). Alerts can be configured (via Prometheus Alertmanager) to notify if, say, a cluster’s training job failed or if a device reported an error.
	•	Application Logs and Traces: All components (Flyte workflows, Flower server, client apps) will emit logs. We will aggregate logs using a centralized log system (e.g., Elastic Stack or Grafana Loki). This helps debugging issues in a distributed context. For instance, if a client crashes during training, the logs (on device or cluster) can be forwarded (with sensitive info removed) to a central place for analysis. We will ensure that log data is also treated carefully (maybe using Presidio to scrub PII from logs too). Using an observability stack like OpenTelemetry, we could even trace a request through the system – e.g., a round ID flows from server to client and back, and we can trace timings. This is advanced but potentially useful if optimizing performance.
	•	Training Metrics and Dashboard: We will keep track of key ML metrics as the training progresses. For example, the server can compute the global model’s loss/accuracy on a validation set at each round or after certain events. Those metrics can be fed into Prometheus as custom metrics (Flower and other FL frameworks often have hooks to get aggregated metrics). Grafana dashboards can then show, for example, “Global accuracy vs. rounds”, “Loss on cluster A’s validation data vs. time”, etc. We also track client-side metrics: each client might evaluate the model on its local data before and after training. These could be reported (in a privacy-preserving way – e.g., aggregated or no raw data). It’s important to monitor not just overall accuracy but fairness or divergence: Are some clients seeing much lower performance than others? Are certain data distributions not improving? We could display distribution of metrics across clients. If the federated updates are causing any instability (like accuracy oscillations), we’ll see that in these dashboards.
	•	Model Drift and Data Drift: Over time, data on clients may change (concept drift). We plan to incorporate drift detection in monitoring. For example, DataHub can store dataset profiles, and if a new profile of data is significantly different (maybe via statistics reported by the client), an alert could be raised that the model may need update (which in FL it naturally will if clients train). Also, if the global model starts to perform worse on an old validation set while improving on new data, that’s an indicator of drift causing forgetting – which we want to catch early (tieing back to continual learning monitoring). We could automate a small job that periodically tests the global model on a suite of benchmark tasks (that represent earlier knowledge) and logs those results. Observing those alongside new task performance lets us quantify trade-offs.
	•	Observability of Workflows: Since Flyte is managing workflows, it comes with a console to view running workflows, their statuses, and logs of each step. This is helpful for operators to see if, say, the “aggregate updates” step failed or the “notify clients” step is hanging. We will expose these or integrate with our monitoring such that if a Flyte workflow fails, it triggers an alert. Flyte also can emit metrics about task duration, queue lengths, etc.
	•	Client Monitoring and App Experience: In cases of mobile apps or IoT, we might also integrate with mobile APM (Application Performance Monitoring) tools to ensure the FL client doesn’t degrade the user’s device experience. For example, monitoring battery usage, memory usage of the client process, etc., and if it’s too high, we adjust the training (maybe smaller batch or wait until charging). While this is a bit beyond typical server monitoring, it’s crucial for the cross-device part to be practical. The FL client could include logic to only run during certain conditions (charging, Wi-Fi). We can gather stats on how often these conditions are met to gauge how many updates we can realistically get from devices.
	•	Security Monitoring: Since multiple parties are involved, we should also monitor security events. Ranger’s audit logs can be monitored for any unusual access attempts (e.g., someone tried to query the data outside of FL). We can set up alerts on those. Also monitor network for any suspicious activity around the FL server (to detect if someone is trying to intrude or if a client is sending malformed updates possibly indicating compromise). Using something like Cloud Armor or other security monitoring if deployed on cloud could help here.

All these monitoring aspects tie into an observability suite that gives us deep visibility into both the machine learning progress and the system’s health. We’ll likely use a combination of Grafana (for visualization), Prometheus (metrics), Loki/Elastic (logs), and possibly custom small dashboards for certain ML metrics not easily charted otherwise. By having this in place, we can answer questions like: Is the training converging?, Are all clusters contributing as expected?, Did any client behave oddly?, What is the throughput of each round?, Are we meeting latency targets?, etc. This is critical for such a complex distributed training framework to run reliably 24/7.

⸻

Conclusion

In summary, this new framework for federated learning brings together powerful tools and concepts to enable collaborative model training across heterogeneous environments. It uses Flower for flexible federated orchestration ￼, PyTorch+DeepSpeed for efficient training of large models across clusters ￼, and LoRA/QLoRA for lightweight fine-tuning on edge devices ￼ ￼. We ensure that data remains siloed and secure through Apache Ranger policies and Presidio’s data anonymization ￼ ￼, so that only learned model knowledge is shared. The framework’s architecture supports both cross-silo and cross-device FL ￼, effectively making it hybrid and highly scalable. By borrowing the Nested Learning paradigm ￼, we treat each client’s training as a nested sub-problem and integrate new knowledge in a way that avoids forgetting old knowledge, facilitating true continual learning. Supporting infrastructure like DataHub and MLflow provides model governance, versioning, and lineage tracking ￼, while Flyte orchestrates the complex multi-step workflows reliably.

This approach will allow us to build and maintain a continually improving foundation model that taps into data distributed worldwide (from servers to smartphones) without ever compromising privacy. Each component we planned – from data pipelines (JSONL datasets, etc.) to monitoring – plays a role in ensuring the system is reliable, secure, and effective. We have essentially outlined a state-of-the-art federated learning platform that combines the latest techniques (like QLoRA, secure aggregation, etc.) with robust MLOps practices. Once implemented, this framework can be the backbone for applications where a global AI model needs to learn from decentralized data (such as personalized assistants, multi-hospital medical models, or IoT analytics) all while keeping data in its rightful place and leveraging the strengths of each tool in our stack.
