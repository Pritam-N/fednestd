# Federated Nested MoE Framework – Milestones Checklist

## Milestone 0 – Repo Hygiene & Baseline

**Goal:** Get the scaffolded project into a clean, buildable, testable state.

- [ ] Rename `pyptoject.toml` → `pyproject.toml` and configure:
  - [ ] Project metadata
  - [ ] Dependencies (torch, deepspeed, typer, kafka-python, etc.)
  - [ ] `console_scripts` entrypoint: `fednestd = fednestd.__main__:main`
- [ ] Ensure `src/fednestd/__main__.py` calls `app()` from `cli.py`
- [ ] Implement basic logging:
  - [ ] `observability/logging.py` with `get_logger`
- [ ] Implement config loading:
  - [ ] `config/loaders.py` with `load_config` (YAML/JSON)
- [ ] Implement minimal stubs:
  - [ ] `infra/deployment_profiles.py` with `load_profile`
  - [ ] `networking/haprozy_config.py` with `render_haproxy_config`
  - [ ] `networking/vpn.py` with `render_vpn_peer_config`
  - [ ] `messaging/kafka_client.py` with `get_admin_client`
  - [ ] `messaging/topics.py` with `bootstrap_topics`
  - [ ] `training/tier1_trainer.py` with `run_core_update`
  - [ ] `training/aggregation.py` with `run_expert_aggregation`
  - [ ] `federation/server.py` with `run_fed_server`
  - [ ] `federation/client.py` with `run_edge_client`
- [ ] Make `fednestd/cli.py` import these stubs and wire all subcommands
- [ ] Set up CI pipeline:
  - [ ] `pip install -e .`
  - [ ] `pytest`
  - [ ] Type checking (pyright/mypy)

**Acceptance criteria**

- [ ] `pip install -e .` works
- [ ] `fednestd --help` and subcommand `--help` all work
- [ ] `pytest` passes
- [ ] Type checker has no unexpected errors


## Milestone 1 – Config & CLI Foundation

**Goal:** Solid CLI + config handling for all main flows.

- [ ] Define Pydantic config models in `config/models.py`:
  - [ ] `Tier1Config`
  - [ ] `Tier2Config`
  - [ ] `MessagingConfig`
  - [ ] `InfraConfig`
- [ ] Update `load_config()` to:
  - [ ] Load YAML/JSON
  - [ ] Map to correct Pydantic model based on `kind`/`mode` field
- [ ] Wire CLI commands to typed configs:
  - [ ] `fednestd tier1 core-update --config ...`
  - [ ] `fednestd tier1 aggregate-experts --config ...`
  - [ ] `fednestd tier1 run-fed-server --config ...`
  - [ ] `fednestd tier2 run-client --config ...`
  - [ ] `fednestd messaging bootstrap-topics --config ...`
  - [ ] `fednestd infra generate-haproxy-config --profile ...`
  - [ ] `fednestd infra generate-vpn-config --profile ...`
  - [ ] `fednestd init-config tier1|tier2`
- [ ] Update `examples/` configs to match Pydantic models
- [ ] Document CLI usage in `docs/cli.md`

**Acceptance criteria**

- [ ] Sample configs generated by `init-config` validate correctly
- [ ] All CLI commands work with sample configs (even if they just log and exit)
- [ ] `docs/cli.md` matches actual CLI behavior


## Milestone 2 – Core Model & Local Training (Non-distributed)

**Goal:** Working MoE model + adapters and a simple local Tier 1 training loop.

- [ ] Implement core MoE model in `model/moe_model.py`:
  - [ ] `W_core` (embeddings, attention, norms)
  - [ ] `W_experts` (expert MLP blocks)
- [ ] Implement adapters in `model/adapters.py`:
  - [ ] LoRA/QLoRA layers on top of expert weights
- [ ] Implement checkpointing in `model/checkpointing.py`:
  - [ ] Save/load checkpoints
  - [ ] Explicitly separate `core`, `experts`, `adapters`
- [ ] Implement basic quantization hooks in `model/quantization.py`
- [ ] Implement a simple single-process training loop in `training/tier1_trainer.py`:
  - [ ] Synthetic or small dataset loader
  - [ ] Loss logging via `get_logger`
  - [ ] Checkpoint writing
- [ ] Update tests:
  - [ ] `tests/test_moe_model.py`
  - [ ] `tests/test_aggregation.py` (smoke tests for aggregation API)

**Acceptance criteria**

- [ ] `fednestd tier1 core-update --config examples/minimal_tier1_cluster.yaml` runs and trains for a few steps locally
- [ ] Checkpoints are written and can be reloadable
- [ ] Tests for `moe_model` and basic training pass


## Milestone 3 – Tier 1 Distributed Training & Metadata Integration

**Goal:** Distributed Tier 1 training with DeepSpeed + basic metadata/governance hooks.

- [ ] Integrate DeepSpeed in `tier1_trainer.run_core_update`:
  - [ ] DeepSpeed config generation or loading
  - [ ] ZeRO + MoE sharding
- [ ] Integrate MLflow:
  - [ ] Configure MLflow tracking URI from config
  - [ ] Log params/metrics/artifacts for Tier 1 runs
  - [ ] Save model checkpoints as artifacts
- [ ] Add governance integration stubs:
  - [ ] `governance/global_ranger.py` to check data access policies (even if stub)
  - [ ] `governance/global_metadata.py` for DataHub/lineage stubs
- [ ] Extend `examples/minimal_tier1_cluster.yaml` for:
  - [ ] DeepSpeed settings
  - [ ] MLflow settings
  - [ ] Governance flags

**Acceptance criteria**

- [ ] Multi-GPU run works (even on synthetic data) via CLI + DeepSpeed
- [ ] MLflow shows runs, metrics, and artifacts
- [ ] Logs clearly indicate Ranger/DataHub hooks are invoked


## Milestone 4 – Messaging & Infra Wiring

**Goal:** Kafka topics and infra helpers (HAProxy, VPN) working in dev environment.

- [ ] Finalize `messaging/kafka_client.py` (producer/consumer/admin helpers)
- [ ] Finalize `messaging/topics.py`:
  - [ ] `DEFAULT_TOPICS` (control, updates, telemetry, tasks)
  - [ ] `bootstrap_topics(config)` implementation
- [ ] Implement `scripts/dev_start_cluster.sh` to:
  - [ ] Start Kafka (docker-compose/k8s)
  - [ ] Start local MLflow (optional)
  - [ ] Any supporting services
- [ ] Finalize `infra/deployment_profiles.py` with proper `TypedDict`s
- [ ] Finalize `networking/haprozy_config.py`
  - [ ] Use `infra/config_templates/haproxy.cfg.j2`
- [ ] Finalize `networking/vpn.py`
  - [ ] Use `infra/config_templates/vpn_peer.conf.j2`
- [ ] Document infra setup in `docs/architecture.md` and `docs/governance.md`

**Acceptance criteria**

- [ ] `fednestd messaging bootstrap-topics --config messaging.yaml` creates topics in local Kafka
- [ ] `fednestd infra generate-haproxy-config` and `generate-vpn-config` produce usable configs
- [ ] `dev_start_cluster.sh` sets up a runnable local environment


## Milestone 5 – Federation Control Plane (Server + Client Skeleton)

**Goal:** Basic federation loop with FedServer and edge client (skeleton logic).

- [ ] Implement `federation/server.py`:
  - [ ] Start Flower/custom FL server
  - [ ] Integrate with Kafka control topic (`control.federation_rounds`)
- [ ] Implement `federation/client.py`:
  - [ ] Connect to FedServer/Kafka
  - [ ] Receive round start / model available events
  - [ ] Download model checkpoint/metadata
  - [ ] Invoke `training.tier2_trainer.run_edge_round` (stub)
- [ ] Wire CLI commands:
  - [ ] `fednestd tier1 run-fed-server --config ...`
  - [ ] `fednestd tier2 run-client --config ...`
- [ ] Adjust example configs:
  - [ ] Tier 1 config with FL server section
  - [ ] Tier 2 config with client, VPN, endpoints

**Acceptance criteria**

- [ ] Dev setup: one Tier1 server + one Tier2 client can talk locally
- [ ] You can see:
  - [ ] Round events being sent/received
  - [ ] Edge client logging that it would train and send deltas (even if stub payloads)


## Milestone 6 – Edge Adapters Training & Local Sidecar Governance

**Goal:** Real LoRA/QLoRA training on edge + enforce local governance via sidecar.

- [ ] Implement adapter training in `training/tier2_trainer.py`:
  - [ ] Load frozen `W_core`, `W_experts` from checkpoint
  - [ ] Attach adapters
  - [ ] Train only adapters on local data
- [ ] Implement sidecar in `governance/local_sidecar.py`:
  - [ ] Policy model (simple YAML/JSON rules or hardcoded for now)
  - [ ] Outbound payload validation (only model deltas)
  - [ ] Local audit logging
  - [ ] Optional DP noise / hashing hooks
- [ ] Integrate sidecar into `federation/client.run_edge_client`:
  - [ ] All deltas pass through sidecar before being published to Kafka
- [ ] Update Tier 2 example config:
  - [ ] Local dataset path
  - [ ] Sidecar policy config

**Acceptance criteria**

- [ ] Edge client:
  - [ ] Receives model
  - [ ] Trains adapters for N steps
  - [ ] Produces real `ΔW_experts_local` payloads
- [ ] Sidecar:
  - [ ] Logs export events locally
  - [ ] Blocks any payloads that are not deltas
- [ ] `updates.experts.local` topic receives delta messages


## Milestone 7 – Aggregation Logic & Nested Learning Behavior

**Goal:** Proper aggregation across tiers with basic nested-learning/anti-forgetting strategy.

- [ ] Implement real aggregation in `training/aggregation.py`:
  - [ ] Consume `ΔW_experts_T1` (if separate)
  - [ ] Consume `ΔW_experts_local` from Kafka
  - [ ] Version-aware weighting (stale updates downweighted)
  - [ ] Optional reliability weighting per client
- [ ] Implement nested learning safeguards:
  - [ ] Regularization between new `W_experts` and previous version
  - [ ] Option for EWC-like penalty or distillation from previous model
- [ ] Extend `training/evaluation.py`:
  - [ ] Global validation metrics
  - [ ] Regression checks vs previous version
- [ ] Wire evaluation & aggregation into CLI or Flyte workflows

**Acceptance criteria**

- [ ] End-to-end: core update + edge updates + aggregation yields a new model version `v+1`
- [ ] Metrics show how `v+1` compares to `v`
- [ ] Aggregation can be run from CLI/Flyte for repeated cycles


## Milestone 8 – Observability & Data Engine Feedback Loop

**Goal:** Good monitoring for the system and a basic data-engine feedback loop.

- [ ] Implement Prometheus metrics in `observability/metrics.py`:
  - [ ] Expose metrics for:
    - Active edge clients
    - Training duration per round
    - Aggregation frequency
    - Kafka consumer lag (if feasible)
- [ ] Add tracing hooks in `observability/tracing.py` (optional with OpenTelemetry)
- [ ] Implement telemetry consumer (“data engine”):
  - [ ] Consume `telemetry.edge`
  - [ ] Identify high-loss or problematic segments
  - [ ] Emit targeted `tasks.training` messages
- [ ] Create Grafana dashboards:
  - [ ] Training metrics
  - [ ] Edge participation
  - [ ] Kafka/topic health

**Acceptance criteria**

- [ ] Grafana dashboards show meaningful metrics from a dev run
- [ ] Telemetry → data-engine → new training tasks loop works in a simple scenario


## Milestone 9 – E2E Integration Scenario (Demo)

**Goal:** Reproducible demo that exercises the entire stack end-to-end.

- [ ] Create a demo script or Make target:
  - [ ] Start infra (Kafka, MLflow, etc.)
  - [ ] Bootstrap topics
  - [ ] Start FedServer + Tier1 trainer
  - [ ] Start 1–N edge clients (containers)
- [ ] Provide example data:
  - [ ] Small JSONL datasets and configs for Tier1 and Tier2
- [ ] Run:
  - [ ] One core update
  - [ ] One or more edge rounds
  - [ ] One aggregation step
- [ ] Verify:
  - [ ] New model version is created
  - [ ] Telemetry & metrics reflect the run

**Acceptance criteria**

- [ ] A new contributor can:
  - [ ] Clone repo
  - [ ] Follow `examples/minimal_tier1_cluster.md`
  - [ ] Run the demo script
  - [ ] Observe logs/metrics and model version evolution


## Milestone 10 – Documentation & v0.1.0 Release

**Goal:** Polish docs and cut a tagged `v0.1.0` release.

- [ ] Finalize docs:
  - [ ] `docs/architecture.md`
  - [ ] `docs/governance.md`
  - [ ] `docs/api_reference.md`
  - [ ] `docs/cli.md`
- [ ] Update `README.md`:
  - [ ] Clear summary, quickstart, architecture overview
- [ ] Ensure docs build (e.g., `mkdocs build` if using mkdocs)
- [ ] Tag release:
  - [ ] Bump version in `pyproject.toml`
  - [ ] Create git tag `v0.1.0`
  - [ ] (Optional) Publish package to internal/external registry

**Acceptance criteria**

- [ ] Docs are consistent with current behavior and architecture
- [ ] A new user can understand and run the system using only README + docs
- [ ] `v0.1.0` tag created and build is reproducible